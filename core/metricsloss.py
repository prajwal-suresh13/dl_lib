
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/core/metricsloss.ipynb
__all__ =['LabelSmoothingCrossEntropy', 'accuracy','mse','acc_seg','dice_score', 'CrossEntropyFlat', 'DiceLoss','gram_matrix',
          'PerceptualLoss']

from dl_lib.core.utils import *
from dl_lib.core.callbacks import *

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, eta = 0.1, reduction='mean'):
        super().__init__()
        self.eta, self.reduction = eta, reduction

    def forward(self, output, target):
        c = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        nll = F.nll_loss(log_preds, target, reduction = self.reduction)

        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)

        return lin_comb(self.eta, loss/c, nll)

def mse(output, target): return (output.squeeze(-1) - target).pow(2).mean()

def accuracy(output, target):
    return (torch.argmax(output, dim=1)==target).float().mean()

def acc_seg(input, target, ignore_mask=None):
    n=target.shape[0]
    target = target.view(n,-1)
    input = input.argmax(dim=1).view(n,-1)
    if ignore_mask is not None:
        mask = target != ignore_mask
        return (input[mask]==target[mask]).float().mean()
    return (input==target).float().mean()

def dice_score(pred, target, smooth=1e-6):
    num_of_classes = pred.shape[1]
    actual_classes = [len(target[i].unique()) for i in range(target.shape[0])]
    actual_classes = tensor(actual_classes).cuda()
    # print(num_of_classes, actual_classes)

    targets = _one_hot(target,num_of_classes)
    # print(targets)
    preds = F.softmax(pred,dim=1)
    # print(preds)
    sum_dims = list(range(2,len(pred.shape)))
    # print(sum_dims)
    intersection = torch.sum(preds*targets,dim=sum_dims)
    # print(intersection)
    union = torch.sum(preds+targets, dim=sum_dims)
    dice_score = ((2. * intersection) / (union )).sum(dim=1)
    dice_score/=actual_classes
    return dice_score.mean()

class CrossEntropyFlat(nn.CrossEntropyLoss):
    def forward(self, input, target):
        n,c,*_ = input.shape
        return super().forward(input.view(n,c,-1), target.view(n,-1))
        #https://discuss.pytorch.org/t/multi-class-semantic-segmentation-using-u-net-error-with-binary-cross-entropy-with-logits/85207

class DiceLoss(nn.Module):
    def __init__(self, smooth=0.):
        super().__init__()
        self.smooth = smooth

    def forward(self, pred, target):
        dice = dice_score(pred,target, self.smooth)
        return 1-dice

def gram_matrix(x):
    n,c,h,w = x.size()
    x = x.view(n, c, -1)
    return (x @ x.transpose(1,2))/(c*h*w)

class PerceptualLoss(nn.Module):
    def __init__(self, m_feat, layer_ids, layer_wgts, scale=0.01):
        super().__init__()
        self.m_feat = m_feat
        self.loss_features = [self.m_feat[i] for i in layer_ids]
        # self.hooks = Hooks(self.loss_features,hook_outputs)
        self.wgts = layer_wgts
        self.mean = tensor([0.485, 0.456, 0.406]).cuda()
        self.std = tensor([0.229, 0.224, 0.225]).cuda()
        self.scale=scale
        # self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
        #       ] + [f'gram_{i}' for i in range(len(layer_ids))]

    def make_features(self, x, clone=False):
        with Hooks(self.loss_features, hook_outputs)as hooks:
            self.m_feat(x)
            return [(h.output.clone() if clone else h.output) for h in hooks]
            # return [print(h.output) for h in hooks]


    def forward(self, input, target):
        input,target= (input/tensor([2.]).cuda())+tensor([0.5]).cuda() ,(target/tensor([2.]).cuda())+tensor([0.5]).cuda()
        input,target = (input-self.mean[...,None,None])/self.std[...,None,None] , (target-self.mean[...,None,None])/self.std[...,None,None]
        out_feat = self.make_features(target, clone=True)
        in_feat = self.make_features(input)
        self.feat_losses = [F.l1_loss(input,target)]
        self.feat_losses += [F.l1_loss(f_in, f_out)*w
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        self.feat_losses += [F.l1_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        # self.metrics = dict(zip(self.metric_names, self.feat_losses))
        self.feat_losses=sum(self.feat_losses) *self.scale
        # print(self.feat_losses)
        return self.feat_losses

